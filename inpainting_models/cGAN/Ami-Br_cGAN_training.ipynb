{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from math import log10\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from tiatoolbox.tools.stainaugment import StainAugmentor\n",
    "import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "wandb.init(project=\"context_encoder_GAN_stain_augmentation_Ami-Br_new\")\n",
    "os.makedirs(\"context_encoder_GAN_stain_augmentation_AMI-Br\", exist_ok=True)\n",
    "\n",
    "class UNetGenerator(nn.Module):\n",
    "    def __init__(self, in_channels=4, out_channels=3):\n",
    "        super(UNetGenerator, self).__init__()\n",
    "\n",
    "        def down_block(in_ch, out_ch, normalize=True):\n",
    "            layers = [nn.Conv2d(in_ch, out_ch, 4, stride=2, padding=1)]\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm2d(out_ch, 0.8))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return nn.Sequential(*layers)\n",
    "\n",
    "        def up_block(in_ch, out_ch, dropout=False):\n",
    "            layers = [nn.ConvTranspose2d(in_ch, out_ch, 4, stride=2, padding=1)]\n",
    "            layers.append(nn.BatchNorm2d(out_ch, 0.8))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "            if dropout:\n",
    "                layers.append(nn.Dropout(0.5))\n",
    "            return nn.Sequential(*layers)\n",
    "\n",
    "        self.d1 = down_block(in_channels, 64, normalize=False)\n",
    "        self.d2 = down_block(64, 128)\n",
    "        self.d3 = down_block(128, 256)\n",
    "        self.d4 = down_block(256, 512)\n",
    "\n",
    "        self.u1 = up_block(512, 256)\n",
    "        self.u2 = up_block(512, 128)\n",
    "        self.u3 = up_block(256, 64)\n",
    "        self.u4 = up_block(128, 64)\n",
    "\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Conv2d(64, out_channels, 3, 1, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        d1_out = self.d1(x)\n",
    "        d2_out = self.d2(d1_out)\n",
    "        d3_out = self.d3(d2_out)\n",
    "        d4_out = self.d4(d3_out)\n",
    "\n",
    "        u1_out = self.u1(d4_out)\n",
    "        u1_out = torch.cat([u1_out, d3_out], dim=1)\n",
    "\n",
    "        u2_out = self.u2(u1_out)\n",
    "        u2_out = torch.cat([u2_out, d2_out], dim=1)\n",
    "\n",
    "        u3_out = self.u3(u2_out)\n",
    "        u3_out = torch.cat([u3_out, d1_out], dim=1)\n",
    "\n",
    "        u4_out = self.u4(u3_out)\n",
    "        return self.final(u4_out)\n",
    "\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, channels=4):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, stride, normalize, dilation=1):\n",
    "            layers = [nn.Conv2d(in_filters, out_filters, 3, stride, 1)]\n",
    "            if normalize:\n",
    "                layers.append(nn.InstanceNorm2d(out_filters))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        layers = []\n",
    "        in_filters = channels\n",
    "        for out_filters, stride, norm, dil in [(64, 2, False,1), (128, 2, True,1), (256, 2, True,1), (512, 2, True, 2),(512, 2, True, 4)]:\n",
    "            layers.extend(discriminator_block(in_filters, out_filters, stride, norm,dil))\n",
    "            in_filters = out_filters\n",
    "\n",
    "        layers.append(nn.AvgPool2d(4))\n",
    "        \n",
    "        layers.append(nn.Conv2d(out_filters, 1, 3, 1, 1))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, img):\n",
    "        return self.model(img)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root, mask_root, img_size=128, mask_size=64, mode=\"train\"):\n",
    "        self.img_size = img_size\n",
    "        self.mask_size = mask_size\n",
    "        self.mode = mode\n",
    "\n",
    "        self.files = sorted(glob.glob(f\"{root}/*.png\"))\n",
    "        if mode == \"train\":\n",
    "            self.files = self.files[:-500]\n",
    "        else:\n",
    "            self.files = self.files[-500:]\n",
    "\n",
    "        self.mask_files = [os.path.join(mask_root, os.path.basename(f)) for f in self.files]\n",
    "\n",
    "        # Stain augmentation parameters\n",
    "        stain_matrix = np.array([\n",
    "            [0.91633014, -0.20408072, -0.34451435],\n",
    "            [0.17669817, 0.92528011, 0.33561059]\n",
    "        ])\n",
    "\n",
    "        if mode == \"train\":\n",
    "            self.augmentation = A.Compose([\n",
    "                A.Resize(img_size, img_size),\n",
    "                A.RandomRotate90(),\n",
    "                A.HorizontalFlip(0.5),\n",
    "                A.VerticalFlip(0.5),\n",
    "                StainAugmentor(method=\"macenko\", stain_matrix=stain_matrix),\n",
    "                A.RandomBrightnessContrast(p=0.2),\n",
    "                A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
    "                ToTensorV2()\n",
    "            ])\n",
    "        else:\n",
    "            self.augmentation = A.Compose([\n",
    "                A.Resize(img_size, img_size),\n",
    "                A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
    "                ToTensorV2()\n",
    "            ])\n",
    "\n",
    "        self.mask_transform = A.Compose([\n",
    "            A.Resize(img_size, img_size, interpolation=1),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "\n",
    "    def apply_random_mask(self, img):\n",
    "        # Finding 0: Might make sense to add border here so MF is not completely visible\n",
    "        border=16\n",
    "        y1, x1 = np.random.randint(border, self.img_size - self.mask_size - border, 2)\n",
    "        y2, x2 = y1 + self.mask_size, x1 + self.mask_size\n",
    "\n",
    "        masked_part = img[:, y1:y2, x1:x2].clone()\n",
    "        masked_img = img.clone()\n",
    "        masked_img[:, y1:y2, x1:x2] = 1\n",
    "        return masked_img, masked_part, y1, x1\n",
    "\n",
    "    def apply_center_mask(self, img):\n",
    "        i = (self.img_size - self.mask_size) // 2\n",
    "        y1, x1 = i, i\n",
    "        y2, x2 = y1 + self.mask_size, x1 + self.mask_size\n",
    "\n",
    "        masked_part = img[:, y1:y2, x1:x2].clone()\n",
    "        masked_img = img.clone()\n",
    "        masked_img[:, y1:y2, x1:x2] = 1\n",
    "        return masked_img, masked_part, y1, x1\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.files[index]\n",
    "        mask_path = self.mask_files[index]\n",
    "\n",
    "        img = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "        mask_img = np.array(Image.open(mask_path).convert(\"L\"))\n",
    "\n",
    "        # Finding 2: The augmentation is independent but should be together\n",
    "        #old:\n",
    "        #augmented = self.augmentation(image=img)\n",
    "        #img = augmented[\"image\"]\n",
    "\n",
    "        #mask_aug = self.mask_transform(image=mask_img)\n",
    "        #mask_img = (mask_aug[\"image\"] > 0.5).float()\n",
    "\n",
    "        #new:\n",
    "        augmented = self.augmentation(image=img, mask=mask_img)\n",
    "        img = augmented[\"image\"]\n",
    "        mask_img = (augmented[\"mask\"] > 0.5).float()[None,:,:]\n",
    "        \n",
    "        if self.mode == \"train\":\n",
    "            masked_img, masked_part, y1, x1 = self.apply_random_mask(img)\n",
    "        else:\n",
    "            masked_img, masked_part, y1, x1 = self.apply_center_mask(img)\n",
    "\n",
    "        return img, masked_img, masked_part, mask_img, y1, x1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "opt = argparse.Namespace(\n",
    "    n_epochs=500,\n",
    "    batch_size=8,\n",
    "    # dataset_name=\"img_align_celeba\",\n",
    "    lr=0.0002,\n",
    "    b1=0.5,\n",
    "    b2=0.999,\n",
    "    n_cpu=4,\n",
    "    # latent_dim=100,\n",
    "    img_size=128,\n",
    "    mask_size=64,\n",
    "    channels=3,\n",
    "    mask_channels=1,\n",
    "    sample_interval=100\n",
    ")\n",
    "\n",
    "wandb.config.update(vars(opt))\n",
    "\n",
    "cuda = torch.cuda.is_available\n",
    "\n",
    "generator = UNetGenerator(in_channels=opt.channels + opt.mask_channels, out_channels=opt.channels)\n",
    "discriminator = Discriminator(channels=opt.channels + opt.mask_channels)\n",
    "#discriminator = ResNet18Discriminator(pretrained=False)\n",
    "\n",
    "if cuda:\n",
    "    generator.cuda()\n",
    "    discriminator.cuda()\n",
    "\n",
    "# Finding 1: Was L2 loss but should be BCE loss IMHO (it's a classification)\n",
    "adversarial_loss = nn.BCEWithLogitsLoss()\n",
    "pixelwise_loss = nn.L1Loss()\n",
    "\n",
    "if cuda:\n",
    "    adversarial_loss.cuda()\n",
    "    pixelwise_loss.cuda()\n",
    "\n",
    "def weights_init_normal(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "\n",
    "generator.apply(weights_init_normal)\n",
    "discriminator.apply(weights_init_normal)\n",
    "\n",
    "train_dataset = ImageDataset(\n",
    "    root=\"/data/Mitosis_Detection/MICCAI25/AMI-Br_dataset/Train_All/Images\",\n",
    "    mask_root=\"/data/Mitosis_Detection/MICCAI25/AMI-Br_dataset/Train_All/Masks\",\n",
    "    img_size=opt.img_size,\n",
    "    mask_size=opt.mask_size,\n",
    "    mode=\"train\"\n",
    ")\n",
    "\n",
    "val_dataset = ImageDataset(\n",
    "    root=\"/data/Mitosis_Detection/MICCAI25/AMI-Br_dataset/Train_All/Images\",\n",
    "    mask_root=\"/data/Mitosis_Detection/MICCAI25/AMI-Br_dataset/Train_All/Masks\",\n",
    "    img_size=opt.img_size,\n",
    "    mask_size=opt.mask_size,\n",
    "    mode=\"val\"\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=opt.batch_size, shuffle=True, num_workers=opt.n_cpu)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=opt.n_cpu)\n",
    "\n",
    "\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "\n",
    "wandb.watch(generator, discriminator, log=\"all\")\n",
    "\n",
    "def save_sample(batches_done):\n",
    "    \"\"\"Save and display a random grid of images, including masks, masked samples, and filled outputs.\"\"\"\n",
    "    val_data = list(val_loader) \n",
    "    random_batch = random.choice(val_data)  \n",
    "    samples, masked_samples, masked_parts, mask_img, y1, x1 = random_batch\n",
    "\n",
    "    samples = samples.type(Tensor)\n",
    "    masked_samples = masked_samples.type(Tensor)\n",
    "    mask_img = mask_img.type(Tensor)\n",
    "    gen_input = torch.cat((masked_samples, mask_img), dim=1)\n",
    "\n",
    "    gen_output = generator(gen_input)\n",
    "\n",
    "    filled_samples = masked_samples.clone()\n",
    "    for b in range(samples.size(0)):\n",
    "        yy1 = y1[b].item()\n",
    "        xx1 = x1[b].item()\n",
    "        filled_samples[b, :, yy1:yy1 + opt.mask_size, xx1:xx1 + opt.mask_size] = \\\n",
    "            gen_output[b, :, yy1:yy1 + opt.mask_size, xx1:xx1 + opt.mask_size]\n",
    "\n",
    "    mask_img_3c = mask_img.repeat(1, 3, 1, 1)\n",
    "\n",
    "    output_grid = torch.cat((mask_img_3c, masked_samples, filled_samples, samples), dim=-2)\n",
    "\n",
    "    grid = make_grid(output_grid, nrow=4, normalize=True, padding=2, pad_value=1)\n",
    "\n",
    "    np_grid = grid.permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "    save_path = f\"context_encoder_GAN_stain_augmentation_AMI-Br/{batches_done}_random.png\"\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.imshow(np_grid)\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig(save_path, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    wandb.log({\"inferred_images\": [wandb.Image(save_path, caption=f\"Step {batches_done} (Random Sample)\")]})\n",
    "\n",
    "def compute_psnr(img1, img2):\n",
    "    img1 = (img1 + 1) / 2\n",
    "    img2 = (img2 + 1) / 2\n",
    "    mse = torch.mean((img1 - img2) ** 2)\n",
    "    if mse.item() == 0:\n",
    "        return 100.0\n",
    "    return 20 * torch.log10(1.0 / torch.sqrt(mse)).item()\n",
    "\n",
    "def compute_ssim(img1, img2):\n",
    "    img1 = (img1.detach().cpu().numpy() + 1) / 2\n",
    "    img2 = (img2.detach().cpu().numpy() + 1) / 2\n",
    "    N, C, H, W = img1.shape\n",
    "    ssim_val = 0.0\n",
    "    for n in range(N):\n",
    "        ssim_c = 0.0\n",
    "        for c in range(C):\n",
    "            ssim_c += ssim(img1[n, c, :, :], img2[n, c, :, :], data_range=1.0)\n",
    "        ssim_val += ssim_c / C\n",
    "    return ssim_val / N\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, masked_imgs, masked_parts, mask_img, y1, x1 = next(iter(train_loader))\n",
    "fig,ax = plt.subplots(8,4, figsize=(10,10))\n",
    "for k in range(imgs.size(0)):\n",
    "    s=torch.Tensor([0.5,0.5,0.5])\n",
    "    m=torch.Tensor([0.5,0.5,0.5])\n",
    "    ax[k,0].imshow(imgs[k].permute(1,2,0).mul_(s).add_(m))\n",
    "    ax[k,1].imshow(masked_imgs[k].permute(1,2,0).mul_(s).add_(m))\n",
    "    ax[k,2].imshow(masked_parts[k].permute(1,2,0).mul_(s).add_(m))\n",
    "    ax[k,3].imshow(mask_img[k].permute(1,2,0))\n",
    "ax[0,0].set_title('imgs')\n",
    "ax[0,1].set_title('masked_imgs')\n",
    "ax[0,2].set_title('masked_parts')\n",
    "ax[0,3].set_title('mask_img')\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, masked_imgs, masked_parts, mask_img, y1, x1 = next(iter(train_loader))\n",
    "\n",
    "gen_input = torch.cat((masked_imgs, mask_img), dim=1)\n",
    "#gen_output = generator(gen_input)\n",
    "gen_input.shape\n",
    "\n",
    "real_filled = masked_imgs.clone()\n",
    "for b in range(imgs.size(0)):\n",
    "    yy1 = y1[b].item()\n",
    "    xx1 = x1[b].item()\n",
    "    real_filled[b, :, yy1:yy1 + opt.mask_size, xx1:xx1 + opt.mask_size] = masked_parts[b]\n",
    "\n",
    "\n",
    "fig,ax = plt.subplots(8,6, figsize=(10,10))\n",
    "for k in range(imgs.size(0)):\n",
    "    s=torch.Tensor([0.5,0.5,0.5])\n",
    "    m=torch.Tensor([0.5,0.5,0.5])\n",
    "    ax[k,0].imshow(gen_input[k][0:3,:,:,].permute(1,2,0).mul_(s).add_(m))\n",
    "    ax[k,1].imshow(gen_input[k][3,:,:,])\n",
    "\n",
    "    yy1 = y1[k].item()\n",
    "    xx1 = x1[k].item()\n",
    "\n",
    "    gen_mask = torch.zeros(size=imgs.shape[2:4])\n",
    "    gen_mask[yy1:yy1 + opt.mask_size, xx1:xx1 + opt.mask_size] = 1\n",
    "    ax[k,2].imshow(gen_mask)\n",
    "    ax[k,3].imshow(masked_parts[k].permute(1,2,0).mul_(s).add_(m))\n",
    "    ax[k,4].imshow(real_filled[k].permute(1,2,0).mul_(s).add_(m))\n",
    "    ax[k,5].imshow(imgs[k].permute(1,2,0).mul_(s).add_(m))\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "ax[0,0].set_title('gen_input [0:3]')\n",
    "ax[0,1].set_title('gen_input [3]')\n",
    "ax[0,2].set_title('mask for inpainting ')\n",
    "ax[0,3].set_title('GT for inpainting ')\n",
    "ax[0,4].set_title('real filled ')\n",
    "ax[0,5].set_title('ref img ')\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_d_losses = []\n",
    "epoch_g_adv_losses = []\n",
    "epoch_g_pixel_losses = []\n",
    "epoch_psnrs = []\n",
    "epoch_ssims = []\n",
    "\n",
    "for epoch in range(opt.n_epochs):\n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "\n",
    "    epoch_d_loss_sum = 0.0\n",
    "    epoch_g_adv_sum = 0.0\n",
    "    epoch_g_pixel_sum = 0.0\n",
    "    n_batches = 0\n",
    "\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        imgs, masked_imgs, masked_parts, mask_img, y1, x1 = batch\n",
    "        imgs = imgs.type(Tensor)\n",
    "        masked_imgs = masked_imgs.type(Tensor)\n",
    "        masked_parts = masked_parts.type(Tensor)\n",
    "        mask_img = mask_img.type(Tensor)\n",
    "        y1 = y1.type(torch.int)\n",
    "        x1 = x1.type(torch.int)\n",
    "\n",
    "#        valid = Tensor(imgs.shape[0], 1, 1, 16).fill_(1.0)\n",
    "#        fake = Tensor(imgs.shape[0], 1, 16, 16).fill_(0.0)\n",
    "        valid = Tensor(imgs.shape[0], 1, 1, 1).fill_(1.0)\n",
    "        fake = Tensor(imgs.shape[0], 1, 1, 1).fill_(0.0)\n",
    "        \n",
    "        optimizer_G.zero_grad()\n",
    "        gen_input = torch.cat((masked_imgs, mask_img), dim=1)\n",
    "        gen_output = generator(gen_input)\n",
    "\n",
    "        g_pixel_loss = 0.0\n",
    "        for b in range(imgs.size(0)):\n",
    "            yy1 = y1[b].item()\n",
    "            xx1 = x1[b].item()\n",
    "            gen_part = gen_output[b:b + 1, :, yy1:yy1 + opt.mask_size, xx1:xx1 + opt.mask_size]\n",
    "            masked_part = masked_parts[b:b + 1]\n",
    "            g_pixel_loss += pixelwise_loss(gen_part, masked_part)\n",
    "        g_pixel_loss /= imgs.size(0)\n",
    "\n",
    "        # Images labeled as real as generator tries to fool discriminator. Alternatively, we could formulate this as maximization.\n",
    "        g_adv_loss = adversarial_loss(discriminator(torch.cat((gen_output, mask_img), dim=1)), valid)\n",
    "\n",
    "        g_loss = 0.1 * g_adv_loss + 1.0 * g_pixel_loss\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "      \n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Question 4: Why real_filled, why not using the original image?\n",
    "        real_filled = masked_imgs.clone()\n",
    "        for b in range(imgs.size(0)):\n",
    "            yy1 = y1[b].item()\n",
    "            xx1 = x1[b].item()\n",
    "            real_filled[b, :, yy1:yy1 + opt.mask_size, xx1:xx1 + opt.mask_size] = masked_parts[b]\n",
    "\n",
    "        real_loss = adversarial_loss(discriminator(torch.cat((imgs, mask_img), dim=1)), valid)\n",
    "        fake_loss = adversarial_loss(discriminator(torch.cat((gen_output.detach(), mask_img), dim=1)), fake)\n",
    "\n",
    "        d_loss = 0.5 * (real_loss + fake_loss)\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        print(\n",
    "            f\"[Epoch {epoch}/{opt.n_epochs}] [Batch {i}/{len(train_loader)}] \"\n",
    "            f\"[D loss: {d_loss.item():.6f}] [G adv: {g_adv_loss.item():.6f}, pixel: {g_pixel_loss.item():.6f}]\"\n",
    "        )\n",
    "\n",
    "        wandb.log({\n",
    "            \"D_loss\": d_loss.item(),\n",
    "            \"G_adv\": g_adv_loss.item(),\n",
    "            \"G_pixel\": g_pixel_loss.item(),\n",
    "            \"epoch\": epoch\n",
    "        })\n",
    "\n",
    "        batches_done = epoch * len(train_loader) + i\n",
    "        if batches_done % opt.sample_interval == 0:\n",
    "            save_sample(batches_done)\n",
    "\n",
    "        epoch_d_loss_sum += d_loss.item()\n",
    "        epoch_g_adv_sum += g_adv_loss.item()\n",
    "        epoch_g_pixel_sum += g_pixel_loss.item()\n",
    "        n_batches += 1\n",
    "\n",
    "    generator.eval()\n",
    "    val_imgs, val_masked_imgs, val_masked_parts, val_mask_img, val_y1, val_x1 = next(iter(val_loader))\n",
    "    val_imgs = val_imgs.type(Tensor)\n",
    "    val_masked_imgs = val_masked_imgs.type(Tensor)\n",
    "    val_mask_img = val_mask_img.type(Tensor)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        gen_input = torch.cat((val_masked_imgs, val_mask_img), dim=1)\n",
    "        gen_output = generator(gen_input)\n",
    "\n",
    "        val_filled = val_masked_imgs.clone()\n",
    "        for b in range(val_imgs.size(0)):\n",
    "            yy1 = val_y1[b].item()\n",
    "            xx1 = val_x1[b].item()\n",
    "            val_filled[b, :, yy1:yy1 + opt.mask_size, xx1:xx1 + opt.mask_size] = gen_output[b, :, yy1:yy1 + opt.mask_size, xx1:xx1 + opt.mask_size]\n",
    "\n",
    "        current_psnr = compute_psnr(val_filled, val_imgs)\n",
    "        current_ssim = compute_ssim(val_filled, val_imgs)\n",
    "\n",
    "    wandb.log({\n",
    "        \"PSNR\": current_psnr,\n",
    "        \"SSIM\": current_ssim,\n",
    "        \"epoch\": epoch\n",
    "    })\n",
    "\n",
    "    epoch_d_losses.append(epoch_d_loss_sum / n_batches)\n",
    "    epoch_g_adv_losses.append(epoch_g_adv_sum / n_batches)\n",
    "    epoch_g_pixel_losses.append(epoch_g_pixel_sum / n_batches)\n",
    "    epoch_psnrs.append(current_psnr)\n",
    "    epoch_ssims.append(current_ssim)\n",
    "\n",
    "torch.save(generator.state_dict(), \"/home/MICCAI25/GAN_AMI-Br/generator_context_encoder_GAN_stain_augmentation.pth\")\n",
    "torch.save(discriminator.state_dict(), \"/home/MICCAI25/GAN_AMI-Br/discriminator_context_encoder_GAN_stain_augmentation.pth\")\n",
    "\n",
    "avg_d_loss = np.mean(epoch_d_losses)\n",
    "avg_g_adv_loss = np.mean(epoch_g_adv_losses)\n",
    "avg_g_pixel_loss = np.mean(epoch_g_pixel_losses)\n",
    "avg_psnr = np.mean(epoch_psnrs)\n",
    "avg_ssim = np.mean(epoch_ssims)\n",
    "\n",
    "print(\"Training Complete!\")\n",
    "print(f\"Average D Loss: {avg_d_loss:.6f}\")\n",
    "print(f\"Average G Adv Loss: {avg_g_adv_loss:.6f}\")\n",
    "print(f\"Average G Pixel Loss: {avg_g_pixel_loss:.6f}\")\n",
    "print(f\"Average PSNR: {avg_psnr:.4f}\")\n",
    "print(f\"Average SSIM: {avg_ssim:.4f}\")\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mitosis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
