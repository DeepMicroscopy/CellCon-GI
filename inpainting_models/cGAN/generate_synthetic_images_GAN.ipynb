{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "\n",
    "class UNetGenerator(nn.Module):\n",
    "    def __init__(self, in_channels=4, out_channels=3):\n",
    "        super(UNetGenerator, self).__init__()\n",
    "\n",
    "        def down_block(in_ch, out_ch, normalize=True):\n",
    "            layers = [nn.Conv2d(in_ch, out_ch, 4, stride=2, padding=1)]\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm2d(out_ch, 0.8))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return nn.Sequential(*layers)\n",
    "\n",
    "        def up_block(in_ch, out_ch, dropout=False):\n",
    "            layers = [nn.ConvTranspose2d(in_ch, out_ch, 4, stride=2, padding=1)]\n",
    "            layers.append(nn.BatchNorm2d(out_ch, 0.8))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "            if dropout:\n",
    "                layers.append(nn.Dropout(0.5))\n",
    "            return nn.Sequential(*layers)\n",
    "\n",
    "        # Downsampling\n",
    "        self.d1 = down_block(in_channels, 64, normalize=False)\n",
    "        self.d2 = down_block(64, 128)\n",
    "        self.d3 = down_block(128, 256)\n",
    "        self.d4 = down_block(256, 512)  # Bottom layer\n",
    "\n",
    "        # Upsampling\n",
    "        self.u1 = up_block(512, 256)\n",
    "        self.u2 = up_block(512, 128)  # after concat with d3\n",
    "        self.u3 = up_block(256, 64)   # after concat with d2\n",
    "        self.u4 = up_block(128, 64)   # after concat with d1\n",
    "\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Conv2d(64, out_channels, 3, 1, 1),\n",
    "            nn.Tanh()  # Output should be in the range of [-1, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Down\n",
    "        d1_out = self.d1(x)\n",
    "        d2_out = self.d2(d1_out)\n",
    "        d3_out = self.d3(d2_out)\n",
    "        d4_out = self.d4(d3_out)\n",
    "\n",
    "        # Up\n",
    "        u1_out = self.u1(d4_out)\n",
    "        u1_out = torch.cat([u1_out, d3_out], dim=1)\n",
    "\n",
    "        u2_out = self.u2(u1_out)\n",
    "        u2_out = torch.cat([u2_out, d2_out], dim=1)\n",
    "\n",
    "        u3_out = self.u3(u2_out)\n",
    "        u3_out = torch.cat([u3_out, d1_out], dim=1)\n",
    "\n",
    "        u4_out = self.u4(u3_out)\n",
    "\n",
    "        return self.final(u4_out)\n",
    "\n",
    "\n",
    "class InferenceDataset(Dataset):\n",
    "    def __init__(self, img_root, mask_root, transforms_, img_size=128, mask_size=64):\n",
    "        self.transform = transforms.Compose(transforms_)\n",
    "        self.img_size = img_size\n",
    "        self.mask_size = mask_size\n",
    "\n",
    "        self.files = sorted(glob.glob(os.path.join(img_root, \"*.png\")))\n",
    "        self.mask_files = [os.path.join(mask_root, os.path.basename(f)) for f in self.files]\n",
    "\n",
    "        self.mask_transform = transforms.Compose([\n",
    "            transforms.Resize((img_size, img_size), Image.NEAREST),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "    def apply_center_mask(self, img):\n",
    "        i = (self.img_size - self.mask_size) // 2\n",
    "        masked_img = img.clone()\n",
    "        masked_img[:, i : i + self.mask_size, i : i + self.mask_size] = 1\n",
    "        return masked_img, i\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.files[index]\n",
    "        mask_path = self.mask_files[index]\n",
    "\n",
    "        img = Image.open(img_path)\n",
    "        img = self.transform(img)\n",
    "\n",
    "        mask_img = Image.open(mask_path).convert(\"L\")\n",
    "        mask_img = self.mask_transform(mask_img)\n",
    "        mask_img = (mask_img > 0.5).float()\n",
    "\n",
    "        masked_img, i = self.apply_center_mask(img)\n",
    "\n",
    "        return img, masked_img, mask_img, i, mask_path  # Return mask path\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "\n",
    "def generate_inpainted_images(\n",
    "    generator_path,\n",
    "    img_folder,\n",
    "    mask_folder,\n",
    "    output_folder,\n",
    "    img_size=128,\n",
    "    mask_size=64,\n",
    "    batch_size=8\n",
    "):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    generator = UNetGenerator(in_channels=4, out_channels=3).to(device)\n",
    "    generator.load_state_dict(torch.load(generator_path, map_location=device))\n",
    "    generator.eval()\n",
    "\n",
    "    transforms_ = [\n",
    "        transforms.Resize((img_size, img_size), Image.BICUBIC),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ]\n",
    "\n",
    "    dataset = InferenceDataset(\n",
    "        img_root=img_folder,\n",
    "        mask_root=mask_folder,\n",
    "        transforms_=transforms_,\n",
    "        img_size=img_size,\n",
    "        mask_size=mask_size,\n",
    "    )\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_i, (imgs, masked_imgs, mask_imgs, center_i, mask_paths) in enumerate(dataloader):\n",
    "            imgs = imgs.to(device)\n",
    "            masked_imgs = masked_imgs.to(device)\n",
    "            mask_imgs = mask_imgs.to(device)\n",
    "\n",
    "            gen_input = torch.cat((masked_imgs, mask_imgs), dim=1)\n",
    "            gen_output = generator(gen_input)\n",
    "\n",
    "            filled_imgs = masked_imgs.clone()\n",
    "            for b in range(imgs.size(0)):\n",
    "                i_val = center_i[b].item()\n",
    "                gen_part = gen_output[b, :, i_val : i_val + mask_size, i_val : i_val + mask_size]\n",
    "                filled_imgs[b, :, i_val : i_val + mask_size, i_val : i_val + mask_size] = gen_part\n",
    "\n",
    "            for b in range(imgs.size(0)):\n",
    "                mask_filename = os.path.basename(mask_paths[b])  # Get mask filename\n",
    "                out_path = os.path.join(output_folder, mask_filename)  # Save with same name\n",
    "\n",
    "                result_img = (filled_imgs[b] * 0.5) + 0.5\n",
    "                save_image(result_img, out_path)\n",
    "\n",
    "                print(f\"Saved: {out_path}\")\n",
    "\n",
    "\n",
    "# Paths for inference\n",
    "generator_path = \"/home/MICCAI25/GAN_CMC/generator_context_encoder_GAN_stain_augmentation.pth\"\n",
    "img_folder = \"/home/MICCAI25/CMC_context_images\"\n",
    "mask_folder = \"/home/MICCAI25/Ami-Br_Train_Augmented_masks\"\n",
    "output_folder = \"/home/MICCAI25/GAN_CMC_Synthetic_Images/Image_CMC_Mask_AMI-Br\"\n",
    "\n",
    "img_size = 128\n",
    "mask_size = 64\n",
    "batch_size = 8\n",
    "\n",
    "generate_inpainted_images(\n",
    "    generator_path=generator_path,\n",
    "    img_folder=img_folder,\n",
    "    mask_folder=mask_folder,\n",
    "    output_folder=output_folder,\n",
    "    img_size=img_size,\n",
    "    mask_size=mask_size,\n",
    "    batch_size=batch_size\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mitosis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
